{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 구현한 방법론의 특징 및 성능\n",
    "\n",
    "### 전처리의 특징\n",
    "1. 특수문자 제거: '····…...↑→'와 같은 특수문자를 제거했다.\n",
    "2. 괄호 안 정보 제거: (), <>, [] 안의 정보는 기사 내용과 크게 연관이 없는 정보이므로 제거했다.\n",
    "3. 한자어는 한글로 변환했다.\n",
    "4. 모든 기사의 헤드라인과 본문 내용을 토큰화하여 vocab으로 저장하되, 그 중 1음절이나 2회 미만출현단어는 제거했다.\n",
    "\n",
    "### 사용한 형태소 분석기\n",
    "Twitter를 사용했다. Twitter와 Mecab에 대해서 기타 조건을 같게 두고 성능을 비교했을 때, Twitter가 근소하게 더 좋은 정확도를 보였기 때문이다. Komoran은 Jpype와 konlpy 사이의 버전 충돌 문제로 인해 사용하지 못했다. \n",
    "\n",
    "||Mecab|Twitter|\n",
    "|-------|-------------------------------|-------------------------------|\n",
    "|vocab 개수|12659|13113|\n",
    "|input_size|1621|1634|\n",
    "|accuracy|12.8125|14.6875|\n",
    "\n",
    "### 사용한 어휘들의 특징 및 vocab 개수\n",
    "Twitter 형태소 분석기를 사용하여 명사만 추출했고, 총 vocab 개수는 13113개 였다.  \n",
    "명사와 동사를 포함하여 vocab을 만들어보았으나, 정확도가 유의미하게 향상되지 않아 명사만 사용하는 것을 선택했다. \n",
    "\n",
    "### 하이퍼패러미터 값\n",
    "activation function: ReLU  \n",
    "batch size: 64 (train 데이터의 개수인 1280의 약수 중 가장 좋은 정확도를 보여서 선택했다)  \n",
    "n_iters: 1280 (여러 번의 시도 끝에 가장 좋은 정확도를 보여서 선택했다)    \n",
    "optimizer: RMSProp  \n",
    "\n",
    "### 성능\n",
    "위와 같이 하이퍼패러미터를 정했을 때, 19.0625 %의 정확도를 보였다.   \n",
    "다음은 모든 조건을 위와 같이 했을 때, optimizer의 종류만을 바꾸며 정확도를 비교하여 표로 정리한 것이다. \n",
    "\n",
    "||SGD|SGD Nesterov|Adam|Adadelta|Adamax|Adagrad|RMSProp|\n",
    "|---------|---------|---------|---------|---------|---------|---------|---------|\n",
    "|accuracy|17.1875|17.5000|17.1875|16.8750|16.2500|16.8750|19.0625|\n",
    "\n",
    "optimizer의 종류와 무관하게 전체적으로 정확도가 매우 낮은 것을 확인할 수 있다.    \n",
    "따라서 더 정교한 모델을 사용하는 방향으로 성능을 높여봐야 한다고 생각된다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import collections\n",
    "import hanja\n",
    "import itertools\n",
    "from konlpy.tag import Twitter\n",
    "from numpy import repeat\n",
    "from konlpy.tag import Mecab\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원데이터를 가공해서 pd dataframe으로 읽기\n",
    "news = pd.read_table(\"newsdata.tsv\", sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 전처리 및 파일을 FNN에 넣기 위한 필요한 모듈 설정\n",
    "\n",
    "# 특수문자 제거하기\n",
    "def remove_special_characters(line):\n",
    "    text = re.sub('[····…...↑→]', '', line)\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "# () 혹은 <> 혹은 [] 안의 내용 제거하기\n",
    "def remove_parentheses(line):\n",
    "    pattern = re.compile(r'(\\(|\\<|\\[).*(\\)|\\>|\\])')\n",
    "    text = re.sub(pattern, '', line)\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "# 한자어는 한글로 변환\n",
    "def hanja_to_hangeul(line):\n",
    "    text = hanja.translate(line, 'substitution')\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "'''\n",
    "# 명사 추출하기 - komoran\n",
    "def extract_nouns_komoran(line):\n",
    "    komoran = Komoran()\n",
    "    text = komoran.nouns(line)\n",
    "    \n",
    "    return text\n",
    "'''\n",
    "\n",
    "\n",
    "# 명사 추출하기 - twitter\n",
    "def extract_nouns_twitter(line):\n",
    "    twt = Twitter()\n",
    "    nouns = twt.nouns(line)\n",
    "    \n",
    "    return nouns\n",
    "\n",
    "\n",
    "# 명사 추출하기 - MeCab\n",
    "def extract_nouns_mecab(line):\n",
    "    mecab = Mecab()\n",
    "    nouns = mecab.nouns(line)\n",
    "    \n",
    "    return nouns\n",
    "\n",
    "\n",
    "# 명사와 동사 추출하기 - twitter\n",
    "def extract_nouns_verbs_twitter(line):\n",
    "    twt = Twitter()\n",
    "    nouns_verbs = list()\n",
    "    for word_pos in twt.pos(line):\n",
    "        if word_pos[1] == 'Noun' or 'Verb':\n",
    "            nouns_verbs.append(word_pos[0])\n",
    "    \n",
    "    return nouns_verbs\n",
    "\n",
    "# 1음절 단어 모으기\n",
    "def get_one_syllable_words(wordlist):\n",
    "    one_syllable_words = []\n",
    "    for word in wordlist:\n",
    "        if len(word) == 1:\n",
    "            one_syllable_words.append(word)\n",
    "    \n",
    "    return one_syllable_words\n",
    "\n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "# 기사 하나에 대해 전처리하기\n",
    "def clean_article(article):\n",
    "    # 특수문자 제거\n",
    "    tokens = remove_special_characters(article)\n",
    "    # 괄호 안의 내용 제거\n",
    "    tokens = remove_parentheses(tokens)\n",
    "    # 한자어는 한글로 변환\n",
    "    tokens = hanja_to_hangeul(tokens)\n",
    "    # 형태소 분석 통해 명사만 추출\n",
    "    tokens = extract_nouns_twitter(tokens)\n",
    "    # tokens = extract_nouns_verbs_twitter(tokens)\n",
    "    # tokens = extract_nouns_mecab(tokens)\n",
    "    # 길이 2 미만의 명사는 제외\n",
    "    tokens = [w for w in tokens if len(w) > 1]\n",
    "\n",
    "    return tokens\n",
    "\n",
    "\n",
    "# 기사 하나에 대해 토큰화하여 vocab으로 저장\n",
    "# inputs: \n",
    "#        article: string\n",
    "#        vocab: collections.Counter\n",
    "def add_article_to_vocab(vocab, article):\n",
    "    # clean article\n",
    "    tokens = clean_article(article)\n",
    "    vocab.update(tokens)\n",
    "\n",
    "    \n",
    "# get article, clean and return a line of tokens\n",
    "# inputs: \n",
    "#        article: string\n",
    "#        vocab: a list of string \n",
    "def article_to_tokens(article, vocab):\n",
    "    # clean article\n",
    "    tokens = clean_article(article)\n",
    "    # filter by vocab\n",
    "    tokens = [w for w in tokens if w in vocab]\n",
    "    return ' '.join(tokens)\n",
    "    \n",
    "    \n",
    "# pandas dataframe 형식의 기사들을 받아 처리하기 (vocab 을 만드는 데 사용함)\n",
    "# inputs: \n",
    "#        vocab: collections.Counter\n",
    "#        df: pandas.dataframe \n",
    "def process_articles_df(vocab, df):\n",
    "    for index, article in df.iterrows():\n",
    "        add_article_to_vocab(vocab, article['headline'] + article['body'])\n",
    "    \n",
    "    \n",
    "# list 형식의 기사들을 받아 처리하기 (개별 기사를 인코딩하는 데 사용함)\n",
    "# inputs: \n",
    "#        vocab: a list of string\n",
    "#        article_list: a list of string(article)    \n",
    "def process_articles(vocab, article_list):\n",
    "    lines = list()\n",
    "    for article in article_list:\n",
    "        line = article_to_tokens(article, vocab)\n",
    "        lines.append(line)\n",
    "    \n",
    "    return lines\n",
    "     \n",
    "    \n",
    "# token_list를 filename에 저장          \n",
    "def save_list(token_list, filename):\n",
    "    data = '\\n'.join(token_list)\n",
    "    file = open(filename,'w')\n",
    "    file.write(data)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/juliejung/ml/env/lib/python3.6/site-packages/konlpy/tag/_okt.py:16: UserWarning: \"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.\n",
      "  warn('\"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.')\n"
     ]
    }
   ],
   "source": [
    "## 전처리 및 vocab 만들기\n",
    "\n",
    "vocab_counter = collections.Counter()\n",
    "process_articles_df(vocab_counter, news)\n",
    "# keep tokens with >= 2 occurrence\n",
    "min_occurance = 2\n",
    "tokens = [k for k,c in vocab_counter.items() if c >= min_occurance]\n",
    "# save tokens to a vocab file\n",
    "save_list(tokens,'vocab.txt')\n",
    "# print(len(tokens)) # 13113"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## encoding, padding, one-hot encoding ...\n",
    "\n",
    "# encoding\n",
    "def texts_to_sequence(train_docs):\n",
    "    # flatten train_docs for encoding\n",
    "    flat_words = list(itertools.chain(*train_docs))\n",
    "    flat_words = sorted(list(set(flat_words)))\n",
    "    \n",
    "    # encode char -> int, decode int-> char\n",
    "    char_to_int = dict((c, i) for i, c in enumerate(flat_words))\n",
    "    \n",
    "    seq_out = list()\n",
    "    for line in train_docs:\n",
    "        encoded_seq = [char_to_int[char] / len(flat_words) for char in line]\n",
    "        seq_out.append(encoded_seq)\n",
    "    return seq_out\n",
    "\n",
    "\n",
    "# padding\n",
    "def pad_sequences(sequences, number, width):\n",
    "    padd_seq_out = list()\n",
    "    for line in sequences:\n",
    "        line.extend(repeat(number, width - len(line)))\n",
    "        padd_seq_out.append(line)\n",
    "    return padd_seq_out\n",
    "      \n",
    "    \n",
    "# one hot encoding\n",
    "def one_hot_encoding(x, number_of_labels):\n",
    "    output = np.zeros([np.size(x), number_of_labels])\n",
    "    for i,index in enumerate(x):\n",
    "        output[i,index]=1\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "# get the length of longest article out of 1600 articles\n",
    "def get_max_width(docs):\n",
    "    max_doc_length = len(docs[0])\n",
    "    for doc in docs:\n",
    "        if len(doc) > max_doc_length:\n",
    "            max_doc_length = len(doc)\n",
    "            \n",
    "    return max_doc_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split new dataframe into train data and test data \n",
    "# x000 ~ x159 goes to train data\n",
    "# x160 ~ x199 goes to test data\n",
    "\n",
    "train_x = [] # a list of string\n",
    "train_y = [] # a list of int\n",
    "\n",
    "test_x = [] # a list of string\n",
    "test_y = [] # a list of int\n",
    "\n",
    "for index, article in news.iterrows():\n",
    "    filename_number = int(article['filename'][1:4])\n",
    "    article_text = article['headline'] + article['body']\n",
    "    article_label = int(article['label'])\n",
    "    if(filename_number in range(160, 200)):\n",
    "        test_x.append(article_text)\n",
    "        test_y.append(article_label)\n",
    "    else:\n",
    "        train_x.append(article_text)\n",
    "        train_y.append(article_label)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/juliejung/ml/env/lib/python3.6/site-packages/konlpy/tag/_okt.py:16: UserWarning: \"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.\n",
      "  warn('\"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.')\n"
     ]
    }
   ],
   "source": [
    "## convert train_x, train_y, test_x, test_y into sequences\n",
    "\n",
    "# load vocabulary\n",
    "vocab_filename = 'vocab.txt'\n",
    "vocab = load_doc(vocab_filename)\n",
    "vocab = vocab.split()\n",
    "vocab = set(vocab)\n",
    "\n",
    "# get the number of tokens for the longest article\n",
    "train_x = process_articles(vocab, train_x) # len(train_x) should be 1280\n",
    "test_x = process_articles(vocab, test_x)\n",
    "max_article_length = get_max_width(train_x + test_x) # 1634\n",
    "# print(max_article_length) # 1634\n",
    "\n",
    "# encode, pad train_x and test_x\n",
    "train_x = pad_sequences(texts_to_sequence(train_x), 0, max_article_length) # (1280, 1634)\n",
    "test_x = pad_sequences(texts_to_sequence(test_x), 0, max_article_length)   # (320, 1634)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x10db8ca10>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set seed\n",
    "\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert train_x, train_y, test_x, test_y to torch.Tensor\n",
    "train_x = torch.Tensor(train_x)\n",
    "train_y = torch.Tensor(train_y)\n",
    "train_y = train_y.type(torch.int64)\n",
    "\n",
    "test_x = torch.Tensor(test_x)\n",
    "test_y = torch.Tensor(test_y)\n",
    "test_y = test_y.type(torch.int64)\n",
    "\n",
    "# convert train_x, train_y to Tensor Dataset\n",
    "train_dataset = torch.utils.data.TensorDataset(train_x, train_y)\n",
    "test_dataset = torch.utils.data.TensorDataset(test_x, test_y)\n",
    "\n",
    "# print(len(train_x), len(test_x)) # 1280 320"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## MAKING DATASET ITERABLE\n",
    "\n",
    "batch_size = 64\n",
    "n_iters = 1280\n",
    "num_epochs = n_iters / (len(train_dataset) / batch_size)\n",
    "num_epochs = int(num_epochs)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size = batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size = batch_size, shuffle=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create Model Class\n",
    "\n",
    "class FFN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim1, hidden_dim2, output_dim):\n",
    "        super(FFN, self).__init__()\n",
    "        # Linear function 1: 1634 --> 100\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim1)\n",
    "        # Non-linearity 1\n",
    "        self.relu1 = nn.ReLU()\n",
    "        \n",
    "        # Linear function 2: 100 --> 50\n",
    "        self.fc2 = nn.Linear(hidden_dim1, hidden_dim2)\n",
    "        # Non-linearity 2\n",
    "        self.relu2 = nn.ReLU()\n",
    "        \n",
    "        # Linear function 3 (readout) : 50 --> 10\n",
    "        self.fc3 = nn.Linear(hidden_dim2, output_dim)  \n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Linear function 1\n",
    "        out = self.fc1(x)\n",
    "        # Non-linearity 1\n",
    "        out = self.relu1(out)\n",
    "        \n",
    "        # Linear function 2\n",
    "        out = self.fc2(out)\n",
    "        # Non-linearity 2\n",
    "        out = self.relu2(out)\n",
    "        \n",
    "        # Linear function 3 (readout)\n",
    "        out = self.fc3(out)\n",
    "        return out      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FFN(\n",
       "  (fc1): Linear(in_features=1634, out_features=100, bias=True)\n",
       "  (relu1): ReLU()\n",
       "  (fc2): Linear(in_features=100, out_features=50, bias=True)\n",
       "  (relu2): ReLU()\n",
       "  (fc3): Linear(in_features=50, out_features=8, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## INSTANTIATE MODEL CLASS\n",
    "\n",
    "input_dim = 1634 # number of tokens in article with the most tokens\n",
    "hidden1_dim = 100\n",
    "hidden2_dim = 50\n",
    "output_dim = 8\n",
    "\n",
    "model = FFN(input_dim, hidden1_dim, hidden2_dim, output_dim)\n",
    "\n",
    "#######################\n",
    "#  USE GPU FOR MODEL  #\n",
    "#######################\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## INSTANTIATE LOSS CLASS\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## INSTANTIATE OPTIMIZER CLASS\n",
    "\n",
    "\n",
    "# SGD\n",
    "# learning_rate = 0.001\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate) #17.1875\n",
    "\n",
    "# Adam\n",
    "# optimizer = torch.optim.Adam(model.parameters()) #17.1875\n",
    "\n",
    "# Adadelta\n",
    "# optimizer = torch.optim.Adadelta(model.parameters()) # 16.875\n",
    "\n",
    "# Adamax\n",
    "# optimizer = torch.optim.Adamax(model.parameters()) # 16.25\n",
    "\n",
    "# RMSProp\n",
    "optimizer = torch.optim.RMSprop(model.parameters()) # 19.0625\n",
    "\n",
    "# Adagrad\n",
    "# optimizer = torch.optim.Adagrad(model.parameters()) # 16.875\n",
    "\n",
    "# SGD Nesterov \n",
    "# learning_rate = 0.01\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, nesterov=True) #17.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 100. Loss: 1.8935710191726685. Accuracy: 15.3125\n"
     ]
    }
   ],
   "source": [
    "## TRAIN THE MODEL\n",
    "\n",
    "iter = 0\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (articles, labels) in enumerate(train_loader):\n",
    "\n",
    "         #######################\n",
    "        #  USE GPU FOR MODEL  #\n",
    "        #######################\n",
    "        articles = articles.view(-1, 1634).requires_grad_().to(device)\n",
    "        labels = labels.to(device = device, dtype=torch.int64)\n",
    "        \n",
    "        # Clear gradients w.r.t. parameters\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass to get output/logits\n",
    "        outputs = model(articles)\n",
    "        \n",
    "        # Calculate Loss: softmax --> cross entropy loss\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Getting gradients w.r.t. parameters\n",
    "        loss.backward()\n",
    "        \n",
    "        # Updating parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        iter += 1\n",
    "        \n",
    "        if iter % 100 == 0:\n",
    "            # Calculate Accuracy         \n",
    "            correct = 0\n",
    "            total = 0\n",
    "            # Iterate through test dataset\n",
    "            for articles, labels in test_loader:\n",
    "                #######################\n",
    "                #  USE GPU FOR MODEL  #\n",
    "                #######################\n",
    "                articles = articles.view(-1, 1634).requires_grad_().to(device)\n",
    "\n",
    "                # Forward pass only to get logits/output\n",
    "                outputs = model(articles)\n",
    "\n",
    "                # Get predictions from the maximum value\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "                # Total number of labels\n",
    "                total += labels.size(0)\n",
    "\n",
    "                #######################\n",
    "                #  USE GPU FOR MODEL  #\n",
    "                #######################\n",
    "                # Total correct predictions    \n",
    "                if torch.cuda.is_available():\n",
    "                    correct += (predicted.cpu() == labels.cpu()).sum()\n",
    "                else:\n",
    "                    correct += (predicted == labels).sum()\n",
    "                \n",
    "            accuracy = 100 * float(correct) / float(total)\n",
    "            \n",
    "            # Print Loss\n",
    "            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
